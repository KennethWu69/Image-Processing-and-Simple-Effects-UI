{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.image as _img\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
    "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
    "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
    "\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### resize\n",
    "file_path = 'test.jpg'\n",
    "img = cv2.imread(file_path)\n",
    "h, w = img.shape[0], img.shape[1]\n",
    "# print(h, w)\n",
    "img = cv2.resize(img, (int(w/10), int(h/10)), interpolation=cv2.INTER_AREA)\n",
    "print(img.shape)\n",
    "cv2.imshow('output', img)\n",
    "# h, w = img.shape[0], img.shape[1]\n",
    "# img = cv2.resize(img, (h*0.2, w*0.2))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "input_batch = transform(img).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "    print(prediction)\n",
    "    \n",
    "\n",
    "    prediction_filter = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=img.shape[:2],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "\n",
    "output = prediction_filter.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Add text on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the fonts locally in a fonts/ directory\n",
    "#georgia_bold = 'fonts/georgia_bold.ttf'\n",
    "georgia_bold_italic = 'I.Ngaan.ttf'\n",
    "\n",
    "txt = input() # text to render\n",
    "txt_len = len(txt)\n",
    "# W, H = (1280, 720) # image size\n",
    "background = (0,0,0) # white\n",
    "fontsize = 24\n",
    "W, H = (fontsize*txt_len, fontsize) # image size\n",
    "font = ImageFont.truetype(georgia_bold_italic, fontsize)\n",
    "\n",
    "image = Image.new('RGBA', (W, H), background)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# w, h = draw.textsize(txt) # not that accurate in getting font size\n",
    "w, h = font.getsize(txt)\n",
    "\n",
    "draw.text(((W-w)/2,(H-h)/2), txt, fill='lime', font=font)\n",
    "# draw.text((10, 0), txt, (0,0,0), font=font)\n",
    "# img_resized = image.resize((188,45), Image.ANTIALIAS)\n",
    "\n",
    "save_location = os.getcwd()\n",
    "\n",
    "# img_resized.save(save_location + '/sample.jpg')\n",
    "image.save(save_location + '/sample.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_img = Image.open('sample.png')\n",
    "rgba = text_img.convert(\"RGBA\")\n",
    "datas = rgba.getdata()\n",
    "  \n",
    "newData = []\n",
    "for item in datas:\n",
    "    if item[0] == 0 and item[1] == 0 and item[2] == 0:  # finding black colour by its RGB value\n",
    "        # storing a transparent value when we find a black colour\n",
    "        newData.append((255, 255, 255, 0))\n",
    "    else:\n",
    "        newData.append(item)  # other colours remain unchanged\n",
    "  \n",
    "rgba.putdata(newData)\n",
    "rgba.save(\"transparent_image.png\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontimage_path = 'transparent_image.png'\n",
    "frontimg = cv2.imread(frontimage_path, cv2.IMREAD_UNCHANGED)\n",
    "h, w = frontimg.shape[0], frontimg.shape[1]\n",
    "frontimg = cv2.resize(frontimg, (int(w), int(h)), interpolation=cv2.INTER_AREA)\n",
    "print(frontimg.shape)\n",
    "# frontimg = cv2.cvtColor(frontimg, cv2.COLOR_BGR2BGRA)\n",
    "cv2.imshow('output', frontimg)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_tmp = img.copy()\n",
    "depth = 10\n",
    "# x_offset = 0\n",
    "# y_offset = 0\n",
    "while True:\n",
    "    img_tmp = img.copy()\n",
    "    x_offset = 150\n",
    "    y_offset = 100\n",
    "    mask = img_tmp[x_offset:x_offset + frontimg.shape[0], y_offset:y_offset + frontimg.shape[1]].copy()\n",
    "    \n",
    "    indices = np.where(output > depth)\n",
    "    # print(indices)\n",
    "    # print(frontimg)\n",
    "\n",
    "    mask[frontimg[:, :, 3] > 0] = frontimg[frontimg[:, :, 3] > 0][:, 0:3]\n",
    "    \n",
    "    \n",
    "    img_tmp[x_offset:x_offset + frontimg.shape[0], y_offset:y_offset + frontimg.shape[1]] = mask\n",
    "    img_tmp[indices] = img[indices]\n",
    "\n",
    "    # indices = np.where(output < depth)\n",
    "    # img_tmp[indices] = (3, 147, 5)\n",
    "    # depth += 0.03\n",
    "    \n",
    "    cv2.imshow('output', img_tmp)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_tmp = img.copy()\n",
    "h, w = img.shape[0], img.shape[1]\n",
    "edge_x = frontimg.shape[0] - h \n",
    "edge_y = frontimg.shape[1] - w\n",
    "\n",
    "depth = 15\n",
    "x_offset = 0\n",
    "y_offset = 0\n",
    "x_offset_add = 1\n",
    "y_offset_add = 1\n",
    "while True:\n",
    "    img_tmp = img.copy()\n",
    "\n",
    "    mask = img_tmp[x_offset:x_offset + frontimg.shape[0], y_offset:y_offset + frontimg.shape[1]].copy()\n",
    "\n",
    "    indices = np.where(output > depth)\n",
    "    # print(indices)\n",
    "    # print(frontimg)\n",
    "\n",
    "    mask[frontimg[:, :, 3] > 0] = frontimg[frontimg[:, :, 3] > 0][:, 0:3]\n",
    "    \n",
    "    \n",
    "    img_tmp[x_offset:x_offset + frontimg.shape[0], y_offset:y_offset + frontimg.shape[1]] = mask\n",
    "    img_tmp[indices] = img[indices]\n",
    "\n",
    "    if x_offset + frontimg.shape[0] >= h:\n",
    "        x_offset_add = -1\n",
    "    elif x_offset <= 0:\n",
    "        x_offset_add = 1\n",
    "    if y_offset + frontimg.shape[1] >= w:\n",
    "        y_offset_add = -1\n",
    "    elif y_offset <= 0:\n",
    "        y_offset_add = 1\n",
    "\n",
    "    x_offset += x_offset_add\n",
    "    y_offset += y_offset_add\n",
    "\n",
    "    cv2.imshow('output', img_tmp)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@article{Ranftl2021,\n",
    "\tauthor    = {Ren\\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},\n",
    "\ttitle     = {Vision Transformers for Dense Prediction},\n",
    "\tjournal   = {ArXiv preprint},\n",
    "\tyear      = {2021},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@article{Ranftl2021,\n",
    "\tauthor    = {Ren\\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},\n",
    "\ttitle     = {Vision Transformers for Dense Prediction},\n",
    "\tjournal   = {ArXiv preprint},\n",
    "\tyear      = {2021},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
